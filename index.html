<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>GyPTix</title>
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <style>
        * {
            box-sizing: border-box;
        }
        :root {
            font-size: 16px;
            --safe-area-top: env(safe-area-inset-top);
            --safe-area-bottom: env(safe-area-inset-bottom);
            --safe-area-left: env(safe-area-inset-left);
            --safe-area-right: env(safe-area-inset-right);
        }
        @media (prefers-color-scheme: dark) {
            :root {
                --background-color: #101010;
                --text-color: #f0f0f0;
            }
        }
        @media (prefers-color-scheme: light) {
            :root {
                --background-color: #ffffff;
                --text-color: #000000;
            }
        }
        html, body {
            -webkit-user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            user-select: none;
        }
        body {
            font-family: sans-serif;
            margin: 0;
            padding: var(--safe-area-top) var(--safe-area-right) var(--safe-area-bottom) var(--safe-area-left);
            background-color: var(--background-color);
            color: var(--text-color);
            font-size: 1rem;
        }
        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.25rem 1rem;
            font-size: 1.2rem;
            font-weight: bold;
        }
        .favicon {
            width: 2rem;
            height: 2rem;
            filter: none;
        }
        .button-container {
            display: flex;
            gap: 0.5rem;
        }
        .button {
            background: none;
            border: none;
            width: 2rem;
            height: 2rem;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1rem;
            cursor: pointer;
            user-select: none;
        }
        .glyphfix {
            border: 0.125rem solid rgba(136, 136, 136, 0.2);
            border-radius: 0.25rem;
        }
        .glyphfix:focus, .glyphfix:active {
            outline: none;
            box-shadow: none;
        }
        .content {
            display: flex;
            max-width: 90%; /* Adjusted to 90% of device width */
            margin: auto;
            padding: 2em;
            gap: 2em;
            font-size: 100%; /* Initial font size for content (1rem) */
        }
        .text {
            flex: 1;
        }
        .qa-section {
            margin-top: 2rem;
        }
        .qa-item {
            margin-bottom: 1rem;
        }
        .qa-question {
            cursor: pointer;
            font-weight: bold;
        }
        .qa-answer {
            display: none;
            margin-top: 0.5rem;
        }
        body {
            background-color: var(--background-color);
        }
        .videos {
            flex: 1;
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        .video-container {
            position: relative;
            width: 50%;
            margin: auto;
        }
        video {
            width: 100%;
        }
        .fullscreen-btn {
            position: absolute;
            bottom: 10px;
            right: 10px;
            background: rgba(0, 0, 0, 0.5);
            color: white;
            border: none;
            padding: 0.5rem;
            cursor: pointer;
        }
        .tabs {
            display: flex;
            justify-content: center;
            gap: 1rem;
            margin-bottom: 1rem;
        }
        .tab {
            cursor: pointer;
            padding: 0.5rem 1rem;
            border: 1px solid #ccc;
            border-radius: 0.25rem;
        }
        .tab.active {
            background-color: #ddd;
        }
        footer {
            text-align: center;
            font-size: 0.8rem;
            padding-top: 1em;
        }
    </style>
</head>
<body>
    <header>
        <img src="favicon.ico" alt="GyPTix Logo" class="favicon">
        <div class="button-container">
            <button id="font-decrease" class="button glyphfix" title="Decrease Font Size" onclick="decreaseFontSize()">a</button>
            <button id="font-increase" class="button glyphfix" title="Increase Font Size" onclick="increaseFontSize()">A</button>
            <button id="toggle_theme" class="button glyphfix" title="Toggle Dark/Light Theme" onclick="toggleTheme()">◐</button>
        </div>
    </header>
    <main>
        <div class="content">
            <div class="text">
                <h3><strong>Your AI, Your Privacy</strong></h3>
                <p>GyPTix is your <strong>private, offline AI companion</strong>—built for iPhone, iPad, and Apple Silicon Macs. No internet? No problem. GyPTix runs <strong>entirely on your device</strong>, keeping your data <strong>safe and private</strong>.</p>
                <p><strong>Privacy First:</strong> No servers. No tracking. What you ask stays on your device.</p>
                <p><strong>Fast & Instant:</strong> No network lag. No waiting. Just quick answers anytime, anywhere.</p>
                <p><strong>Free & Open-Source:</strong> No subscriptions, no ads—just AI at your fingertips.</p>
                <hr>
                <h3><strong>Perfect for Anytime, Anywhere</strong></h3>
                <p>Stuck on a plane? No Wi-Fi? GyPTix is there.</p>
                <p>Need quick brainstorming? Get instant answers.</p>
                <p>Value your privacy? Your data <strong>never</strong> leaves your device.</p>
                <hr>
                <h3><strong>Apple Hardware. Apple Speed.</strong></h3>
                <p>Powered by <strong>Apple’s Neural Engine</strong>, GyPTix is designed to <strong>run fast</strong> on your iPhone, iPad, or Mac. No delays, just <strong>smooth performance</strong> without the cloud.</p>
                <p><strong>Optimized for Apple Silicon</strong></p>
                <p><strong>Runs on-device local AI model</strong></p>
                <p><strong>Future-ready with Mixture of Experts (MoE) models and reasoning</strong></p>
                <hr>
                <h3><strong>Why GyPTix?</strong></h3>
                <p><strong>No subscriptions</strong></p>
                <p><strong>No data collection</strong></p>
                <p><strong>Just pure AI, ready when you need it.</strong></p>
                <hr>
                <p><strong>Download Now – It’s Free!</strong></p>
                <p><a href="https://tinyurl.com/GyPTix-iOS"><img src="AppStore-light.svg" alt="Download on the App Store" width="150"></a></p>
                <p><a href="https://tinyurl.com/GyPTix-macOS"><img src="MacAppStore-light.svg" alt="Download on the Mac App Store" width="150"></a></p>
                <p><a href="https://tinyurl.com/GyPTix-TestFlight"><strong>Join TestFlight for Early Access</strong></a></p>
                <hr>                
                <div class="qa-section" id="qa-section"></div>
            </div>
            <div class="videos">
                <div class="tabs">
                    <div class="tab active" onclick="showVideo('ios')">iOS</div>
                    <div class="tab" onclick="showVideo('macos')">macOS</div>
                </div>
                <div id="ios-video" class="video-container">
                    <video src="iOS-640x480-dark.mp4" controls></video>
                    <button class="fullscreen-btn" onclick="toggleFullscreen(this)">Fullscreen</button>
                </div>
                <div id="macos-video" class="video-container" style="display: none;">
                    <video src="macOS-640x480-light.mp4" controls></video>
                    <button class="fullscreen-btn" onclick="toggleFullscreen(this)">Fullscreen</button>
                </div>
            </div>
        </div>
    </main>
    <footer>
        &copy; 2025 GyPTix
    </footer>
    <script>
        function toggleTheme() {
            let theme = localStorage.getItem("theme") || "auto";
            if (theme === "dark") {
                setTheme("light");
            } else {
                setTheme("dark");
            }
        }
        function setTheme(mode) {
            const root = document.documentElement;
            localStorage.setItem("theme", mode);
            if (mode === "dark") {
                document.body.style.backgroundColor = "#101010";
                document.body.style.color = "#f0f0f0";
                document.querySelectorAll(".glyphfix").forEach(el => el.style.filter = "invert(1) grayscale(1)");
            } else {
                document.body.style.backgroundColor = "#ffffff";
                document.body.style.color = "#000000";
                document.querySelectorAll(".glyphfix").forEach(el => el.style.filter = "none");
            }
        }
        function loadTheme() {
            const savedTheme = localStorage.getItem("theme");
            if (savedTheme) {
                setTheme(savedTheme);
            } else {
                setTheme(window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light");
            }
        }
        function increaseFontSize() {
            const content = document.querySelector('main') || document.querySelector('.content');
            let computedStyle = window.getComputedStyle(content);
            let fontSize = parseFloat(computedStyle.fontSize);
            let rootFontSize = parseFloat(window.getComputedStyle(document.documentElement).fontSize);
            let fontSizePercent = (fontSize / rootFontSize) * 100;
            if (fontSizePercent < 150) { // Max scale 150%
                content.style.fontSize = (fontSizePercent + 10) + '%';
            }
        }
        function decreaseFontSize() {
            const content = document.querySelector('main') || document.querySelector('.content');
            let computedStyle = window.getComputedStyle(content);
            let fontSize = parseFloat(computedStyle.fontSize);
            let rootFontSize = parseFloat(window.getComputedStyle(document.documentElement).fontSize);
            let fontSizePercent = (fontSize / rootFontSize) * 100;
            if (fontSizePercent > 75) { // Min scale 75%
                content.style.fontSize = (fontSizePercent - 10) + '%';
            }
        }
        function showVideo(video) {
            document.getElementById('ios-video').style.display = 'none';
            document.getElementById('macos-video').style.display = 'none';
            document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
            if (video === 'ios') {
                document.getElementById('ios-video').style.display = 'block';
                document.querySelector('.tab:nth-child(1)').classList.add('active');
            } else {
                document.getElementById('macos-video').style.display = 'block';
                document.querySelector('.tab:nth-child(2)').classList.add('active');
            }
        }
        function toggleFullscreen(button) {
            const videoContainer = button.closest('.video-container');
            if (!document.fullscreenElement) {
                videoContainer.requestFullscreen().catch(err => {
                    alert(`Error attempting to enable fullscreen mode: ${err.message} (${err.name})`);
                });
            } else {
                document.exitFullscreen();
            }
        }
        function toggleAnswer(event) {
            const answer = event.target.nextElementSibling;
            if (answer.style.display === "none" || !answer.style.display) {
                answer.style.display = "block";
            } else {
                answer.style.display = "none";
            }
        }
        document.addEventListener("DOMContentLoaded", () => {
            loadTheme();
            const qaData = [
                {
                    question: "What is on-device AI and why is it becoming increasingly significant?",
                    answer: "On-device AI refers to the deployment and execution of artificial intelligence models directly on local devices like smartphones, tablets, and embedded systems, rather than relying on cloud-based servers. This trend is gaining significance due to several key advantages: enhanced user privacy by keeping data on the device, the ability for applications to function offline without an internet connection, faster and more responsive performance due to reduced latency from eliminating network communication, and lower operational costs by decreasing reliance on cloud infrastructure."
                },
                {
                    question: "What hardware components are crucial for enabling on-device AI, particularly for complex tasks like running large language models (LLMs)?",
                    answer: "Neural Processing Units (NPUs), also known as Neural Engines, Tensor Processing Units, or AI Accelerators, are specialized hardware components designed to accelerate the matrix multiplications and other calculations fundamental to AI and deep learning workloads. These NPUs work alongside CPUs and GPUs within a System on a Chip (SoC) to efficiently handle the intense computational demands of on-device AI, including running increasingly capable LLMs. The performance of an NPU is often measured in TOPS (Trillions of Operations Per Second), indicating its ability to perform these calculations rapidly."
                },
                {
                    question: "How are large language models being adapted for on-device use, considering the limited resources of mobile and edge devices?",
                    answer: "Adapting LLMs for on-device processing involves various optimization techniques to reduce their size and computational requirements without significant loss of performance. These methods include model quantization (reducing the precision of numerical representations), knowledge distillation (training a smaller model to mimic a larger, more capable one), structural pruning (removing less important connections in the neural network), and the use of smaller, more efficient model architectures like the Phi-3 family, Gemma, Ministral, and Octopus V2. Additionally, techniques like dynamic weight loading and sliding window approaches help manage memory constraints by loading only necessary parts of the model into faster memory during inference."
                },
                {
                    question: "What are some of the current applications and potential future use cases of on-device AI in consumer applications and platforms like Android and Chrome?",
                    answer: "Current applications of on-device AI include features like enhanced photo and video processing, real-time language translation, intelligent assistants, personalized recommendations, and offline chatbot capabilities (as seen with Flutter and Gemma). In platforms like Android, Google's Gemini Nano enables generative AI tasks directly on the device. Future use cases include more advanced and context-aware digital assistants, improved accessibility features, proactive and personalized user experiences, on-device analysis of documents and media, and enhanced privacy-preserving features leveraging local processing. Chrome is also integrating on-device AI like Gemini Nano to enable features such as live translation and chat prediction directly within the browser."
                },
                {
                    question: "What are the trade-offs and challenges associated with deploying AI models on devices compared to cloud-based AI?",
                    answer: "Deploying AI on devices presents trade-offs primarily related to computational power, model size, and the need for efficient resource utilization. On-device AI models are typically smaller and less computationally intensive than their cloud-based counterparts, which can sometimes lead to a reduction in accuracy or the range of tasks they can perform. Challenges include managing limited memory and battery life, optimizing models for specific hardware, and handling the diversity of devices with varying capabilities. While on-device AI offers benefits like privacy and offline functionality, cloud-based AI can leverage significantly more computational resources and larger models, potentially offering more complex and accurate results. Hybrid solutions that combine on-device processing for speed and basic tasks with cloud computation for more intensive operations are also being explored."
                },
                {
                    question: "How do frameworks and tools like Hyperapp, TensorFlow Lite, MediaPipe, and vLLM facilitate the development and deployment of on-device AI applications?",
                    answer: "Frameworks and tools play a crucial role in simplifying the development and deployment of on-device AI. TensorFlow Lite and MediaPipe provide optimized libraries and tools for running machine learning models on mobile and edge devices. Hyperapp, while a JavaScript library for building web applications, demonstrates the principles of state management and UI rendering which are relevant when building user interfaces that interact with on-device AI. vLLM is a library focused on efficient inference of large language models and can be used in offline mode, making it suitable for on-device LLM applications where resources are constrained. These tools help developers convert and optimize models, manage device resources, and build applications that effectively leverage on-device AI capabilities."
                },
                {
                    question: "Which companies are at the forefront of developing hardware and software solutions for on-device AI?",
                    answer: "Several major technology companies are leading the charge in on-device AI. Apple designs its own SoCs with dedicated NPUs (Neural Engines) integrated into their iPhones and Macs. Google develops Tensor Processing Units (TPUs) for both cloud and on-device applications (like in Pixel phones with the Tensor G3 chip) and is integrating models like Gemini Nano into Android and Chrome. Qualcomm is a major provider of mobile chipsets with integrated NPUs, supporting on-device AI capabilities in a wide range of Android devices. Samsung has also announced its entry into the AI field with its 'Gauss' generative AI model, focusing on on-device applications. Additionally, companies like ARM Holdings, which designs chip blueprints used by many mobile chipmakers, are crucial in advancing the efficiency and AI capabilities of mobile processors. Microsoft with its Phi-3 family optimized for Copilot+ PCs and Intel with its Meteor Lake architecture incorporating NPUs also play significant roles."
                },
                {
                    question: "What are the potential implications of widespread on-device AI for data privacy and security, and how might user control over personal data evolve?",
                    answer: "Widespread on-device AI has significant positive implications for data privacy and security. By processing data locally on the device, it reduces the need to send sensitive information to the cloud, thereby minimizing the risks associated with data transmission and storage on remote servers. This shift aligns with a growing desire for user control over personal data. Concepts like data permits and systems that encrypt and decentralize personal data storage could be further enabled by on-device AI, allowing users to have greater autonomy over how their information is used. While on-device AI enhances privacy, it also necessitates careful consideration of security on the device itself to prevent unauthorized access to models and locally processed data."
                }
            ];
            const qaSection = document.getElementById("qa-section");
            qaData.forEach(item => {
                const qaItem = document.createElement("div");
                qaItem.className = "qa-item";
                const question = document.createElement("div");
                question.className = "qa-question";
                question.textContent = item.question;
                question.addEventListener("click", toggleAnswer);
                const answer = document.createElement("div");
                answer.className = "qa-answer";
                answer.textContent = item.answer;
                qaItem.appendChild(question);
                qaItem.appendChild(answer);
                qaSection.appendChild(qaItem);
            });
        });
        localStorage.clear(); /* DEBUG initial theme */
    </script>
</body>
</html>
